{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\r\n",
        "\r\n",
        "SparkSession.builder.getOrCreate()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "xml_path = 'abfss://raw1xml@synstrg.dfs.core.windows.net/*.xml' #NCT05504733"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ctpool",
              "session_id": "29",
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-09-24T10:57:54.5481406Z",
              "session_start_time": null,
              "execution_start_time": "2022-09-24T10:57:54.7519131Z",
              "execution_finish_time": "2022-09-24T10:57:54.9179513Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(ctpool, 29, 3, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format(\"com.databricks.spark.xml\").option(\"rowTag\",\"clinical_study\").option(\"attributePrefix\",\"__\").load(xml_path)\r\n",
        "df.count()\r\n",
        "#display(df)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ctpool",
              "session_id": "29",
              "statement_id": 4,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-09-24T10:58:00.5111843Z",
              "session_start_time": null,
              "execution_start_time": "2022-09-24T10:58:00.6328893Z",
              "execution_finish_time": "2022-09-24T10:58:38.1129552Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(ctpool, 29, 4, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "560"
          },
          "metadata": {}
        }
      ],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, explode\r\n",
        "\r\n",
        "\r\n",
        "def type_cols(df_dtypes, filter_type):\r\n",
        "    cols = []\r\n",
        "    for col_name, col_type in df_dtypes:\r\n",
        "        if col_type.startswith(filter_type):\r\n",
        "            cols.append(col_name)\r\n",
        "    return cols"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ctpool",
              "session_id": "29",
              "statement_id": 5,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-09-24T10:58:46.9006991Z",
              "session_start_time": null,
              "execution_start_time": "2022-09-24T10:58:47.0165424Z",
              "execution_finish_time": "2022-09-24T10:58:47.1937868Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(ctpool, 29, 5, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "struct_cols = type_cols(df.dtypes,\"struct\")\r\n",
        "\r\n",
        "struct_cols"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "array_cols = type_cols(df.dtypes,\"array\")\r\n",
        "\r\n",
        "array_cols"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\r\n",
        "\r\n",
        "\r\n",
        "def flatten_df1(nested_df):\r\n",
        "    stack = [((), nested_df)]\r\n",
        "    columns = []\r\n",
        "\r\n",
        "    while len(stack) > 0:\r\n",
        "        parents, df = stack.pop()\r\n",
        "\r\n",
        "        flat_cols = [\r\n",
        "            col(\".\".join(parents + (c[0],))).alias(\"_\".join(parents + (c[0],)))\r\n",
        "            for c in df.dtypes\r\n",
        "            if c[1][:6] != \"struct\"\r\n",
        "        ]\r\n",
        "\r\n",
        "        nested_cols = [\r\n",
        "            c[0]\r\n",
        "            for c in df.dtypes\r\n",
        "            if c[1][:6] == \"struct\"\r\n",
        "        ]\r\n",
        "\r\n",
        "        columns.extend(flat_cols)\r\n",
        "\r\n",
        "        for nested_col in nested_cols:\r\n",
        "            if nested_col != 'clinical_results':\r\n",
        "                projected_df = df.select(nested_col + \".*\")\r\n",
        "                stack.append((parents + (nested_col,), projected_df))\r\n",
        "\r\n",
        "    return nested_df.select(columns)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ctpool",
              "session_id": "29",
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-09-24T10:59:03.4760205Z",
              "session_start_time": null,
              "execution_start_time": "2022-09-24T10:59:03.639276Z",
              "execution_finish_time": "2022-09-24T10:59:04.5586891Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(ctpool, 29, 6, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 27,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_flat =flatten_df1(df)\r\n",
        "\r\n",
        "display(df_flat.limit(10))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', 'False')\r\n",
        "\r\n",
        "pd_df = df_flat.toPandas()\r\n",
        "\r\n",
        "print(pd_df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd_df.to_csv('abfss://syncstrg@synstrg.dfs.core.windows.net/output/clinical_trial.csv', header= True, sep=',')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ctpool",
              "session_id": "29",
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-09-24T11:00:38.2208394Z",
              "session_start_time": null,
              "execution_start_time": "2022-09-24T11:00:38.3313756Z",
              "execution_finish_time": "2022-09-24T11:00:46.710451Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(ctpool, 29, 9, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 30,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd_df.to_excel('abfss://syncstrg@synstrg.dfs.core.windows.net/output/clinical_trial.xlsx', header= True)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ctpool",
              "session_id": "29",
              "statement_id": 10,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-09-24T11:00:57.3462285Z",
              "session_start_time": null,
              "execution_start_time": "2022-09-24T11:00:57.461521Z",
              "execution_finish_time": "2022-09-24T11:01:01.5213701Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(ctpool, 29, 10, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 31,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/output/flattenoutput\"\r\n",
        "df_flat.write.format(\"parquet\").mode(\"overwrite\").save(path)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ctpool",
              "session_id": "29",
              "statement_id": 11,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-09-24T11:01:25.7858674Z",
              "session_start_time": null,
              "execution_start_time": "2022-09-24T11:01:25.8918028Z",
              "execution_finish_time": "2022-09-24T11:02:15.4364668Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(ctpool, 29, 11, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 32,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sing = df_flat.coalesce(1)\r\n",
        "\r\n",
        "path_sing = \"/output/singleparquet\"\r\n",
        "\r\n",
        "df_sing.write.format(\"parquet\").mode(\"overwrite\").save(path_sing)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ctpool",
              "session_id": "29",
              "statement_id": 12,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-09-24T11:03:56.0578229Z",
              "session_start_time": null,
              "execution_start_time": "2022-09-24T11:03:56.2941404Z",
              "execution_finish_time": "2022-09-24T11:04:42.3801894Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(ctpool, 29, 12, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 33,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}